#!/usr/bin/env python3
from p2p.train import run
import click


@click.group()
def p2p():
    pass

@p2p.command()
@click.option('--fasta-file',
              help='Input sequences in fasta format.')
@click.option('--links-directory',
              help='Directory of tab-delimited files of interactions.')
@click.option('--checkpoint-path',
              help='Checkpoint path.')
@click.option('--data-dir',
              help='Directory of pretrained data.')
@click.option('--model-path',
              help='Output model path.')
@click.option('--logging-path',
              help='Logging directory.', default=None)
@click.option('--training-column',
              help='Training column in links file.', default='Training')
@click.option('--embedding-dimension',
              help='Number of embedding dimensions to model interactions', default=100)
@click.option('--num-neg',
              help='Number of negative samples.', default=2)
@click.option('--epochs',
              help='Number of epochs for training.', default=10)
@click.option('--learning-rate',
              help='Learning rate.', default=5e-5)
@click.option('--warmup-steps',
              help='Warmup steps for scheduler.', default=0)
@click.option('--gradient-accumulation-steps',
              help=('Number of steps before gradients are computed. '
                   'This allows for a larger effective batch size.'), 
              default=0)
@click.option('--fp16',
              help='16 bit precision.', default=False)
@click.option('--batch-size',
              help='Number of sequences per batch for training per GPU.', default=10)
@click.option('--num-workers',
              help='Number of workers', default=10)
@click.option('--summary-interval',
              help='Summary interval in seconds', default=7200)
@click.option('--checkpoint-interval',
              help='Checkpoint interval in seconds', default=7200)
@click.option('--arm-the-gpu', is_flag=True,
              help='Specifies whether or not to use the GPU.', default=False)
def interactions(fasta_file, links_directory,
                 checkpoint_path, data_dir, model_path, logging_path,
                 training_column, embedding_dimension, num_neg, epochs,
                 learning_rate, warmup_steps, gradient_accumulation_steps, fp16,
                 batch_size, num_workers, summary_interval, checkpoint_interval, 
                 arm_the_gpu):
    if arm_the_gpu:
        # pick out the first GPU
        device_name = 'cuda'
    else:
            device_name = 'cpu'
    print('links_directory', links_directory)
    run(fasta_file, links_directory, 
        checkpoint_path, data_dir,
        model_path, logging_path,
        training_column=training_column,
        emb_dimension=embedding_dimension,
        num_neg=num_neg, epochs=epochs, 
        learning_rate=learning_rate, 
        warmup_steps=warmup_steps, 
        gradient_accumulation_steps=gradient_accumulation_steps,
        batch_size=batch_size, num_workers=num_workers,
        summary_interval=summary_interval, 
	checkpoint_interval=checkpoint_interval,
        device=device_name)

if __name__ == "__main__":
    interactions()
