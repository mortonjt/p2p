#!/usr/bin/env python3
from p2p.train import run


@click.group()
def p2p():
    pass

@p2p.command()
@click.option('--fasta-file',
              help='Input sequences in fasta format.')
@click.option('--links-file',
              help='Input tab-delimited file of interactions.')
@click.option('--checkpoint-path',
              help='Checkpoint path.')
@click.option('--data-dir',
              help='Directory of pretrained data.')
@click.option('--model-path',
              help='Output model path.')
@click.option('--logging-path',
              help='Logging directory.', default=None)
@click.option('--training-column',
              help='Training column in links file.', default='Training')
@click.option('--num-neg',
              help='Number of negative samples.', default=10)
@click.option('--batch-size',
              help='Number of sequences per batch for training.', default=10)
@click.option('--num-workers',
              help='Number of workers', default=10)
@click.option('--arm-the-gpu',
              help='Specifies whether or not to use the GPU.', default=False)
def finetune(fasta_file, links_file,
             checkpoint_path, data_dir, model_path, logging_path,
             training_column, num_neg, batch_size, num_workers, arm_the_gpu):

    run(fasta_file, links_file,
        checkpoint_path, data_dir, model_path, logging_path,
        training_column, num_neg, batch_size, num_workers, arm_the_gpu)


if __name__ == "__main__":
    finetune()
